{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "75730fec-0ac2-402c-aa07-4d485c543ba9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<module 'rpbuild.generation' from '/home/dinalt/rust/ai_development/roleplay_build/rpbuild/generation.py'>"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "import time\n",
    "import importlib\n",
    "import transformers\n",
    "from datasets import load_dataset, load_from_disk\n",
    "import torch\n",
    "import re\n",
    "\n",
    "import rpbuild as rp\n",
    "import rpbuild.data\n",
    "import rpbuild.generation\n",
    "\n",
    "\n",
    "# Trigger dynamic reload of module -- for editing without restarting the kernel\n",
    "importlib.reload(rp.generation)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a2c583d-c85f-408f-a866-468b8471b7a8",
   "metadata": {},
   "source": [
    "### Load Resources\n",
    "Load dataset and model for testing..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0f790029-1654-4072-8837-da877c6b4208",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1d937607998541038f3af9674e12b7db",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Resolving data files:   0%|          | 0/543 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "413fa1943f1e488a8e74dfb588df5b4f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating train split: 0 examples [00:00, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset({\n",
      "    features: ['preset', 'summary', 'pairing_reason', 'plist', 'conversation', 'scenario', 'greeting', 'example_dialog', 'char_name', 'description', 'director_log', 'proxy'],\n",
      "    num_rows: 4349\n",
      "})\n",
      "Tokenizer uses \"right\" padding; this may require moving it to \"left\" for batch generation.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`low_cpu_mem_usage` was None, now set to True since model is quantized.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b0344814988c4e14a94ae503ec6d2a73",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/8 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MistralForCausalLM(\n",
      "  (model): MistralModel(\n",
      "    (embed_tokens): Embedding(32000, 4096, padding_idx=2)\n",
      "    (layers): ModuleList(\n",
      "      (0-31): 32 x MistralDecoderLayer(\n",
      "        (self_attn): MistralFlashAttention2(\n",
      "          (q_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
      "          (k_proj): Linear4bit(in_features=4096, out_features=1024, bias=False)\n",
      "          (v_proj): Linear4bit(in_features=4096, out_features=1024, bias=False)\n",
      "          (o_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
      "          (rotary_emb): MistralRotaryEmbedding()\n",
      "        )\n",
      "        (mlp): MistralMLP(\n",
      "          (gate_proj): Linear4bit(in_features=4096, out_features=14336, bias=False)\n",
      "          (up_proj): Linear4bit(in_features=4096, out_features=14336, bias=False)\n",
      "          (down_proj): Linear4bit(in_features=14336, out_features=4096, bias=False)\n",
      "          (act_fn): SiLU()\n",
      "        )\n",
      "        (input_layernorm): MistralRMSNorm()\n",
      "        (post_attention_layernorm): MistralRMSNorm()\n",
      "      )\n",
      "    )\n",
      "    (norm): MistralRMSNorm()\n",
      "  )\n",
      "  (lm_head): Linear(in_features=4096, out_features=32000, bias=False)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "from transformers import BitsAndBytesConfig\n",
    "\n",
    "# Where are models stored?\n",
    "models_dir = \"/home/dinalt/ai_assets/models\"\n",
    "\n",
    "# Configure a model to use.\n",
    "# The name of this model -- which will live in models_dir\n",
    "model_name = \"fhai50032_RolePlayLake-7B\" # AKA \"fhai50032/RolePlayLake-7B\"\n",
    "model_id = os.path.join(models_dir, model_name)\n",
    "\n",
    "# Or... or load it from the hub / cache\n",
    "#model_id = \"fhai50032/RolePlayLake-7B\",\n",
    "\n",
    "# The location of the input dataset\n",
    "dataset_id = \"/home/dinalt/rust/datasets/roleplay_dialog/\"\n",
    "\n",
    "# Device to run model on\n",
    "device = 0\n",
    "\n",
    "# Load dataset\n",
    "dataset = load_dataset(dataset_id)[\"train\"]\n",
    "print(dataset)\n",
    "\n",
    "# Load model with quantization\n",
    "# See link for configuration options alternatives\n",
    "# https://huggingface.co/docs/transformers/main/en/quantization\n",
    "\n",
    "# Load model and tokenizer\n",
    "causal_lm = rp.CausalLM(\n",
    "    model_id,\n",
    "    device=None,\n",
    "    torch_dtype=torch.bfloat16,\n",
    "    attn_implementation=\"flash_attention_2\",\n",
    "    #device_map=\"auto\",\n",
    "    quantization_config=BitsAndBytesConfig(\n",
    "        load_in_4bit=True,\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08ad5203-fdeb-4506-86a5-779d8f290845",
   "metadata": {},
   "source": [
    "## Main Test Harness\n",
    "This is a mock for the automation pytorch script interface.\n",
    "\n",
    "There is an output class, in this case, the \"OutputChars\" object, which merely makes a list of the output dictionaries for inspection.\n",
    "\n",
    "An actual generator script would change the implementation to write the generated data to disk."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "d42e1e8f-1749-491e-85cc-d74688ee9cab",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|                                                                                                                                                                                                           | 0/1 [00:00<?, ?it/s]Token indices sequence length is longer than the specified maximum sequence length for this model (2516 > 255). Running this sequence through the model will result in indexing errors\n",
      "  0%|                                                                                                                                                                                                           | 0/1 [01:29<?, ?it/s]\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'FIX_DIALOG_EXAMPLES' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[12], line 29\u001b[0m\n\u001b[1;32m     27\u001b[0m \u001b[38;5;66;03m# Test inference engine.\u001b[39;00m\n\u001b[1;32m     28\u001b[0m start \u001b[38;5;241m=\u001b[39m time\u001b[38;5;241m.\u001b[39mperf_counter()\n\u001b[0;32m---> 29\u001b[0m \u001b[43mrp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgeneration\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43minfer\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     30\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;66;43;03m# Fake local rank\u001b[39;49;00m\n\u001b[1;32m     31\u001b[0m \u001b[43m    \u001b[49m\u001b[43mlocal_rank\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     32\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdataset\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdataset\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mshuffle\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     33\u001b[0m \u001b[43m    \u001b[49m\u001b[43msampler\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msampler\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     34\u001b[0m \u001b[43m    \u001b[49m\u001b[43mgenerator\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgeneration\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgenerate_dialog\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     35\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;66;43;03m# Dialog generator does not presently support batch inference for size other that 1\u001b[39;49;00m\n\u001b[1;32m     36\u001b[0m \u001b[43m    \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     37\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_fn\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_chars\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     38\u001b[0m \n\u001b[1;32m     39\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;66;43;03m# Steps before calling output_fn\u001b[39;49;00m\n\u001b[1;32m     40\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_steps\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m4\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     41\u001b[0m \n\u001b[1;32m     42\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;66;43;03m# Limit steps, for testing.\u001b[39;49;00m\n\u001b[1;32m     43\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmax_steps\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     44\u001b[0m \u001b[43m    \u001b[49m\u001b[43mgenerator_kwargs\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;28;43mdict\u001b[39;49m\u001b[43m(\u001b[49m\n\u001b[1;32m     45\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcausal_lm\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcausal_lm\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     46\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdataset\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdataset\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     47\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmax_tokens\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m2000\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     48\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     49\u001b[0m \u001b[43m)\u001b[49m\n\u001b[1;32m     50\u001b[0m end \u001b[38;5;241m=\u001b[39m time\u001b[38;5;241m.\u001b[39mperf_counter()\n\u001b[1;32m     51\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mElapsed \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m(end\u001b[38;5;241m-\u001b[39mstart)\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m.03f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m secs.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[0;32m~/rust/ai_development/roleplay_build/rpbuild/generation.py:155\u001b[0m, in \u001b[0;36minfer\u001b[0;34m(local_rank, dataset, sampler, generator, generator_kwargs, batch_size, output_fn, output_steps, max_steps)\u001b[0m\n\u001b[1;32m    153\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m global_step, batch \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(tqdm\u001b[38;5;241m.\u001b[39mtqdm(data_loader, total\u001b[38;5;241m=\u001b[39mmax_steps)):\n\u001b[1;32m    154\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m process_batch:\n\u001b[0;32m--> 155\u001b[0m         output_batch \u001b[38;5;241m=\u001b[39m \u001b[43mgenerator\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbatch\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mgenerator_kwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    156\u001b[0m         output_records \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m output_batch\n\u001b[1;32m    157\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m is_output_step(global_step, output_steps):\n",
      "File \u001b[0;32m~/rust/ai_development/roleplay_build/rpbuild/generation.py:95\u001b[0m, in \u001b[0;36mgenerate_dialog\u001b[0;34m(batch, causal_lm, dataset, max_tokens)\u001b[0m\n\u001b[1;32m     92\u001b[0m character[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdirector_log\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m char\u001b[38;5;241m.\u001b[39mdirector_log\n\u001b[1;32m     94\u001b[0m \u001b[38;5;66;03m# Fix example dialog\u001b[39;00m\n\u001b[0;32m---> 95\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[43mFIX_DIALOG_EXAMPLES\u001b[49m:\n\u001b[1;32m     96\u001b[0m     character[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mexample_dialog\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m start_token_re\u001b[38;5;241m.\u001b[39msplit(character[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mexample_dialog\u001b[39m\u001b[38;5;124m\"\u001b[39m])[\u001b[38;5;241m1\u001b[39m:]\n\u001b[1;32m     98\u001b[0m \u001b[38;5;66;03m# Attach proxy-user meta, as to allow recreation\u001b[39;00m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'FIX_DIALOG_EXAMPLES' is not defined"
     ]
    }
   ],
   "source": [
    "from torch.utils.data.distributed import DistributedSampler\n",
    "from torch.utils.data import SequentialSampler\n",
    "import time\n",
    "\n",
    "sampler = SequentialSampler(dataset)\n",
    "\n",
    "# Mock output class\n",
    "# This just appends the output items to a list.\n",
    "# See class rp.CharacterWriter for a json implementation\n",
    "class OutputChars():\n",
    "    def __init__(self):\n",
    "        self.chars = []\n",
    "\n",
    "    def __call__(self, local_rank, global_step, output_records):\n",
    "        self.chars += output_records\n",
    "\n",
    "    def exists(self, local_rank, global_step):\n",
    "        return False\n",
    "    \n",
    "    def file_path(self, local_rank, global_step):\n",
    "        return \"null\"\n",
    "\n",
    "output_chars = OutputChars()\n",
    "\n",
    "transformers.set_seed(44) # 44\n",
    "\n",
    "# Test inference engine.\n",
    "start = time.perf_counter()\n",
    "rp.generation.infer(\n",
    "    # Fake local rank\n",
    "    local_rank=0,\n",
    "    dataset=dataset.shuffle(),\n",
    "    sampler=sampler,\n",
    "    generator=rp.generation.generate_dialog,\n",
    "    # Dialog generator does not presently support batch inference for size other that 1\n",
    "    batch_size=1,\n",
    "    output_fn=output_chars,\n",
    "\n",
    "    # Steps before calling output_fn\n",
    "    output_steps=4,\n",
    "\n",
    "    # Limit steps, for testing.\n",
    "    max_steps=1,\n",
    "    generator_kwargs = dict(\n",
    "        causal_lm=causal_lm,\n",
    "        dataset=dataset,\n",
    "        max_tokens=2000,\n",
    "    )\n",
    ")\n",
    "end = time.perf_counter()\n",
    "print(f\"Elapsed {(end-start):.03f} secs.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ddf6f9d2-c250-4c39-b82c-68a9073e0bb7",
   "metadata": {},
   "source": [
    "### Dump Output From Mock\n",
    "\n",
    "Dump the list of generated dictionaries producted by the mock."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6013213d-2346-420b-b158-f9550edd897b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dump characters from output\n",
    "def dump_characters(characters):\n",
    "    for i, character in enumerate(characters):\n",
    "        print(f\"\\n{i:-^80}\")\n",
    "        rp.dump_character_data(character)\n",
    "        \n",
    "\n",
    "dump_characters(output_chars.chars)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
